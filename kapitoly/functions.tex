\chapter{Implementation}

In this chapter the description of testing Python scripts is provided as well as comment on functions used in testing scripts. Note that int he following function definitions respective docstrings are omitted as they provide similar information as the descriptions, but in a 	briefer version.      

\section{Testing Scripts Description}

\section{Function Description}
\begin{lstlisting}[caption=]

\end{lstlisting}

The function \tn{}


% -------------------------- XML parsing -------------------------- 
\subsection*{Functions for Extracting Ground Truth}

I created the following functions in order to extract ground truth information from  text files -- in most cases XML files. Each dataset unfortunately uses a different form of annotations. The styles differ in order of data, tag names, even in type of the recorded data (sometimes individual characters and their position is written or information about curvature). For XML parsing I utilized the The ElementTree XML API, which is imported as xml.etree.ElementTree. Information in XML files is stored in a tree where individual values can be accessed gradually using tags and their attributes.

All functions return annotations in the following format :
\newline \tn{(label, [[top left X, top left Y], [bottom right X, bottom right Y]])}, a tuple of label and bounding rectangle coordinates in an array. 
If the images in the dataset were scaled then coordinates in annotations need to be scaled with same ratio. This ratio can be passed as a function argument and is default to one.

\begin{lstlisting}[caption=read\_gt\_ctw\_test]
def read_gt_ctw_test(data, scaling_ratio=1):
    # one line = one bounding polygon : list of coordinates, each separated by commas, last is the text inside 
    # there are #### before each text, two additional ## no text recognized

    annotations = []
    with open(data, "r") as file:
        for line in file:
            line = line.rstrip('\n')
            text = line.split("####")
            label = text[-1]
            coordinates = text[0].split(",")[:-1]
            c = [int(i) for i in coordinates]
            minX = min(c[::2])*scaling_ratio
            maxX = max(c[::2])*scaling_ratio
            minY = min(c[1::2])*scaling_ratio
            maxY = max(c[1::2])*scaling_ratio

            bbox_coords = np.array( [[minX, minY], [maxX, maxY]] )
            annotations.append((label, bbox_coords))

    return annotations
\end{lstlisting}

The function \tn{read\_gt\_ctw\_test} reads TXT files that contains the annotations for testing part of the SCUT-CTW1500 dataset.


\begin{lstlisting}[caption=read\_gt\_ctw\_train]
def read_gt_ctw_train(xml_file, scaling_ratio=1):
    gt = []

    tree = ET.parse(xml_file)
    root = tree.getroot()

    # get values in this order: height, left coordinate, top coordinate, width
    for i, bbox in enumerate(root[0].findall('box')):
        # create list of integers with bounding box values, sort by attribute name
        # in case in different document there is a different order of attributes
        bbox_integer = [int(val) for key, val in sorted(bbox.attrib.items(), key = lambda el: el[0])]
        
        # calculate bottom coordinate of bounding rectangle x+width, y+height
        x_right= int((bbox_integer[1] + bbox_integer[3]) * scaling_ratio)
        y_bottom = int((bbox_integer[2] + bbox_integer[0]) * scaling_ratio)
        x_left = int(bbox_integer[1] * scaling_ratio)
        y_top = int(bbox_integer[2] * scaling_ratio)

        bbox_coords = np.array([[x_left, y_top], [x_right, y_bottom]])

        # get label
        label = root[0][i].find('label').text

        # create list of labels and corresponding bounding boxes
        gt.append((label, bbox_coords))

    return gt
\end{lstlisting}

The function \tn{read\_gt\_ctw\_train} reads the annotations from  XML files for training part of the SCUT-CTW1500 dataset.

\begin{lstlisting}[caption=read\_gt\_kaist]
def read_gt_kaist(xml_file, scaling_ratio=1):
    gt = []

    tree = ET.parse(xml_file)
    root = tree.getroot()

    # some files has images as root tag, 
    # some image as root tag (one tree layer (tag images) is missing)
    try:
        image_tag = root[0][2]
    except IndexError:
        image_tag = root[2]

    # get values in this order: height, width, x (left) coordinate, y (top) coordinate
    for i, bbox in enumerate(image_tag.findall('word')):
        # create list of integers with bounding box values, sort by attribute name
        # in case in different document there is a different order of attributes
        bbox_integer = [int(val) for key, val in sorted(bbox.attrib.items(), key = lambda el: el[0])]
        
        # calculate bottom coordinate of bounding rectangle x+width, y+height
        x_right= int((bbox_integer[2] + bbox_integer[1]) * scaling_ratio)
        y_bottom = int((bbox_integer[3] + bbox_integer[0]) * scaling_ratio)
        x_left = int(bbox_integer[2] * scaling_ratio)
        y_top = int(bbox_integer[3] * scaling_ratio)

        bbox_coords = np.array([[x_left, y_top], [x_right, y_bottom]])

        # get label
        label = ""
        for char in image_tag[i].findall('character'):
            ch = char.get('char')
            label += ch
        # create list of labels and corresponding boundin boxes
        gt.append((label, bbox_coords))

    return gt
\end{lstlisting}

The function \tn{read\_gt\_kaist} extract annotations from XML files with labels for KAIST Scene Text dataset.

\begin{lstlisting}[caption=read\_gt\_bd]
def read_gt_bd(data, scaling_ratio=1):
    annotations = []

    with open(data, "r", encoding='utf-8-sig') as file:
        for line in file:
            line = line.rstrip('\n')
            text = line.split(",")
            bbox_coords = np.array(
                [[int(text[0])*scaling_ratio, int(text[1])*scaling_ratio], 
                [int(text[4])*scaling_ratio, int(text[5])*scaling_ratio]])
            annotations.append((text[-1], bbox_coords))

    return annotations

\end{lstlisting}

Annotations for the Born-Digital dataset are stored in simple TXT file, where there are only four coordinates of the four corners of each bounding rectangle and a word within the rectangele. These values are processed by the function \tn{read\_gt\_bd}.

\begin{lstlisting}[caption=read\_gt\_wien]
def read_gt_wien(xml_file, scaling_ratio=1, resized_previously=False):
    # poster dataset
    # returns labels in a tuple - first contains coordinates (8 numbers), second word (string)

    tree = ET.parse(xml_file)
    root = tree.getroot()
    image_width = root.find('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}Page').get('imageWidth')

    if resized_previously:
        scaling_ratio = scaling_ratio / int(image_width)

    root.iter('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextRegion')

    annotations = []
    for word in root.iter('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}Word'):
        coordinates = word.find('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}Coords').get('points')
        text = word.find('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextEquiv')
        # extract only top left and bottom right coordinates and apply scale
        coords = coordinates.split(" ")
        topL = coords[0].split(",")
        bottomR = coords[2].split(",")
        bbox_coords = np.array(
                    [[int(int(topL[0])*scaling_ratio), int(int(topL[1])*scaling_ratio)], 
                    [int(int(bottomR[0])*scaling_ratio), int(int(bottomR[1])*scaling_ratio)]])
        annotations.append((text[0].text, bbox_coords))

    return annotations
\end{lstlisting}

The function \tn{read\_gt\_wien} takes the label and coordinate values form a XML files for the Vienna City Poster Dataset. The XML files produced by the software Aletheia are in a format created by PRiMA Research. When going through the XML tree an information about the research organization is embodied in the tag names.

\subsection*{Functions for Editing Images}

\begin{lstlisting}[caption=image\_text\_crop]
def image_text_crop(images, filenames, ground_truth, one_file=True, result_folder='./results', skip_longer_than=40):
    # test if there are not more gts than images
    # else the for loop will never get to those exceeding image count
    gt_length = len(ground_truth)
    if len(images) > gt_length:
        images = images[:gt_length]

    if not os.path.isdir(result_folder):
        os.mkdir(result_folder)

    all_texts = []
    for i, img in tqdm(enumerate(images)):
        name, ext = os.path.splitext(filenames[i])

        # count regions in one image - used for file naming purposes
        region = 1
        
        for text, bbox in ground_truth[i]:
            if text is None:
                continue
            if len(text) > skip_longer_than:
                continue
            # select image within coordinates (bbox)
            cropped = img[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]

            # create image file:
            # name in format "original-00region.ext"
            new_name = name + '-' + str(region).zfill(3)
            
            if np.size(cropped):
                cv.imwrite(os.path.join(result_folder, new_name + ext), cropped)
                # create  text annotation file(s)
                if one_file:
                    all_texts.append(new_name + ext + '\t' + text)
                else:
                    # one file for each image with word
                    all_texts.append(new_name + ext + '\t' + text)
                    with open(os.path.join(result_folder, new_name + '.gt.txt'), 'w') as f:
                        f.write(text)
                region += 1

    if one_file:
        with open(os.path.join(result_folder, 'gt.txt'), 'w') as f:
            for line in all_texts:
                f.writelines(line+'\n')

    return all_texts
\end{lstlisting}

The function \tn{image\_text\_crop} crops and saves images based on bounding boxes provided in ground truth for each text region in an image. This is done for all images in \tn{images} list. Also a text file is created with a corresponding text annotation. The name of the resulted file bears the original image name and a number is attached for each text region. The boolean parameter \tn{one\_file} allows to save ground truths only to a single file. Each line consists then of the name of the cropped image and its annotation. The parameter \tn{skip\_longer\_than} skips ground truths that have more characters than stated, because some OCR tools (such as keras-OCR) does not support long strings.

\begin{lstlisting}[caption=shrink\_all]
def shrink_all(images, width):
    scaled = []
    
    for image in images:
        if image.shape[1] > width:
            ratio = width / image.shape[1]
            height = int(image.shape[0] * ratio)
            new_size = (width, height)  
            scaled.append(cv.resize(image, new_size, interpolation=cv.INTER_AREA))
        else:
            scaled.append(image)
    return scaled    
\end{lstlisting}

The function \tn{shrink\_all} returns a list of resized images to a given width. Images already smaller than width are kept unaffected. The resizing aspects ratio.



\begin{lstlisting}[caption=bounding\_rectangle]
def bounding_rectangle(coordinates):
    x, y = zip(*coordinates)

    return np.array([[int(min(x)), int(min(y))], [int(max(x)), int(max(y))]])
\end{lstlisting}

The function \tn{bounding\_rectangle}
Returns top left and bottom right coordinates of a rectangle, that is circumscribed to a polygon defined by coordinates. These are obtained from predictions for each text region.

\subsection*{Functions for Computing Metrics}

% -------------------------- IOU --------------------------  

\begin{lstlisting}[caption=iou]
def iou(pred_box, gt_box):
    # find intersection rectangle coordinates
    x_left = max(pred_box[0][0], gt_box[0][0])
    x_right = min(pred_box[1][0], gt_box[1][0])
    y_top = max(pred_box[0][1], gt_box[0][1])
    y_bottom = min(pred_box[1][1], gt_box[1][1])

    if x_right < x_left or y_bottom < y_top:
        return 0
    
    # compute intersection area
    intersection = (x_right - x_left) * (y_bottom - y_top)

    # compute union area
    pred_area = (pred_box[1][0] - pred_box[0][0]) * (pred_box[1][1] - pred_box[0][1]) 
    gt_area = (gt_box[1][0] - gt_box[0][0]) * (gt_box[1][1] - gt_box[0][1]) 
    union  = pred_area + gt_area - intersection

    # return iou
    return  intersection/union

\end{lstlisting}

The function \tn{iou} computes intersection over union of two bounding boxes, as defined in \ref*{sec:eval}. Both input parameters have to contain top left and bottom right coordinates of a bounding box rectangle.

\begin{lstlisting}[caption=iou\_image]
def iou_image(pred_boxes, gt_boxes):
    ious = []

    # have to determine which prediction bounding box contains same (similar) 
    #  text region as ground truth bounding box
    # find and save the best iou for a prediction box and gt box
    for pred_ind, pred in enumerate(pred_boxes):
        max_iou = 0
        max_ind = 0
        for gt_ind, gt in enumerate(gt_boxes):
            iou_value = iou(pred, gt)
            if (iou_value > max_iou):
                max_iou = iou_value
                max_ind = gt_ind

        # match words from prediction and ground thruth (indeces)     
        ious.append((max_iou, pred_ind, max_ind))

    return ious

\end{lstlisting}

The function \tn{iou\_image} computes intersection over union for all text regions in one image.
Each parameter shall contain a list of two coordinates - (top left, bottom right) of a bounding box rectangle.
Returns a list of tuples - each tuple consists of the highest iou value (thus having the biggest overlap), the index of predicted bounding box and  the index of ground truth bounding box. The indeces are taken from the given lists of bounding boxes or ground truths belonging to one particular image.

\begin{lstlisting}[caption=group\_text]
def group_text(lst):
    grouped = []
    key = lambda x: x[2]

    for k, g in itertools.groupby(sorted(lst, key=key), key):
        list_data = list(zip(*g))
        grouped.append((sum(list_data[0]), list_data[1], k))

    return grouped
\end{lstlisting}

The function \tn{group\_text} is a utility function for matching corresponding strings in a list. The list that is passed as an argument is a list of tuples of IOUs returned from the function \tn{iou\_image}. First for each ground truth bounding box, which is determined by the ground truth index (in this case the third element of the tuple) and it is used as a key. The returned tuple consists of a sum of IOUs, a list of indeces of predicted bounding boxes and ground truth index.

% -------------------------- Text Comparision -------------------------- 


\begin{lstlisting}[caption=compare\_text\_cer]
def compare_text_cer(text, special_characters=False, case_sensitive=False, spaces=True, split=True):
    text_gt, text_pred = text
    # remove special characters and case sensitivity if necessary
    if not spaces:
        text_gt = "".join(char for char in text_gt if (char.isalnum()))
        text_pred = "".join(char for char in text_pred if (char.isalnum()))
    elif not special_characters:
        text_gt = "".join(char for char in text_gt if (char.isalnum() or char.isspace()))
        text_pred = "".join(char for char in text_pred if (char.isalnum() or char.isspace()))
    if not case_sensitive:
        text_gt = text_gt.lower()
        text_pred = text_pred.lower()
    
    corresponding_words = []

    if split:
        words_gt = text_gt.split(" ")
        words_pred = text_pred.split(" ")

        # list of words that are corresponding (based on levenshtein distance)
        # and cer value. (=tuple of three elements)
        # for every predicted word find its corresponding gt wordle
        for word_pred in words_pred:    
            min_dist = (1000, (0, 0))
            min_gt_word = ""                  
            for word_gt in words_gt: 
                l_dist = levenshtein_distance(word_gt, word_pred)
                if l_dist[0] < min_dist[0]:
                    min_dist = l_dist
                    min_gt_word = word_gt
            # count normalized cer (the result will be from 0 to 1), 1 is the worst
            # for computation we devide Levenshtein dist. by sum 
            # of the length of the word and count of insertions performed
            if len(min_gt_word) > 0 and len(word_pred) > 0:
                cer = min_dist[0] / (len(min_gt_word) + min_dist[1][2])
            else:
                cer = 1
            corresponding_words.append((min_gt_word, word_pred, cer))

        # no split of words    
    else:
        l_dist = levenshtein_distance(text_gt, text_pred)

        if len(text_gt) > 0 and len(text_pred) > 0:
            cer = l_dist[0] / (len(text_gt) + l_dist[1][2])
        else:
            cer = 1
        corresponding_words.append((text_gt, text_pred, cer))

    return sorted(corresponding_words)
\end{lstlisting}

The function \tn{compare\_text\_cer} returns a sorted list of corresponding words. Each pair of corresponding words is represented as a tuple of ground truth string, predicted string and a CER value. The first parameter have to be a tuple of a single ground truth string and a predicted string. The argument \tn{special\_characters} ignores all characters except alphanumeric characters and space when \tn{False}. This is applied to both predicted and ground truth strings. When the argument \tn{case\_sensitive} is \tn{False} then all texts are set to lowercase. The \tn{spaces} parameter allows only alphanumeric characters and all spaces are removed. The last one of arguments is \tn{split} and it is used optionally depending on the OCR engine and on dataset. When \tn{True}, strings from ground truth and prediction are split with space used as a separator. Then the Levenshtein distance is computed for each pair of ground truth and predicted word. The best one is chosen for each predicted word and returned together with the two closest words. In case of no split, the Levenshtein distance is computed directly for the strings in the original tuple \tn{text}. It is recommended to use split when the OCR model tends to detect words that belongs to each other as separate words and ground truth marks them as a text line with multiple words. Or when ground truth is one-word and model predicts strings with multiple words together.

The function \tn{levenshtein\_distance} computes the Levenshtein distance (definition can be found in section \ref*{sec:eval}). The implementation is taken from a console application called xer. The code is available from \url{https://github.com/jpuigcerver/xer/blob/master/xer}. It returns a tuple of counts of the three performed operations --  substitution, deletion, insertion.

% -------------------------- Testing Scripts -------------------------- 
\subsection*{Functions Used in Testing Scripts}

\begin{lstlisting}[caption=]

\end{lstlisting}

The function \tn{}


% -------------------------- Final Visualization -------------------------- 
\subsection*{Functions for Final Visualizations}

\begin{lstlisting}[caption=get\_results]
def get_results(filenames, iou_in_image, cer_in_image):
    df_results = pd.DataFrame(list(zip(filenames, iou_in_image, cer_in_image)), columns =['Filename', 'IoU', 'CER'])
    mean_iou = round(df_results['IoU'].mean() * 100, 1)
    mean_cer = round((1 - df_results['CER'].mean()) * 100, 1)
    
    print(f"mean IoU accuracy = {mean_iou}%, mean CER accuracy = {mean_cer}%")
    display(df_results)
    
    return mean_iou, mean_cer, df_results
    
\end{lstlisting}

The function \tn{get\_results} prints returns average IOU and CER for the whole dataset and a dataframe with these metrics for each image.


\begin{lstlisting}[caption=plot\_results]
def plot_results(image, ground_truth, predicted, size=15):
    # Create figure and axes
    figure, ax = plt.subplots(figsize=(size, size))

    # Display the image
    ax.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB), cmap=plt.get_cmap('gray'))
    ax.axis('off')

    for label, bbox  in ground_truth:
        topleft = bbox[0]
        height = bbox[1,1] - bbox[0,1]
        width = bbox[1,0] - bbox[0,0]

        # create and add rectangle
        rect = patches.Rectangle((topleft), width, height, linewidth=1, edgecolor='g', facecolor='none')
        ax.add_patch(rect)

        # add labels
        ax.text(topleft[0]+width, topleft[1],label,verticalalignment='top', color='g',fontsize=13, bbox=dict(facecolor='g', alpha=0.2, edgecolor='g'))

    for label, bbox  in predicted:
        topleft = bbox[0]
        height = bbox[1,1] - bbox[0,1]
        width = bbox[1,0] - bbox[0,0]

        # create and add rectangle
        rect = patches.Rectangle((topleft), width, height, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

        # add labels
        ax.text(topleft[0]-2, topleft[1]-5,label,verticalalignment='top', color='r',fontsize=13, bbox=dict(facecolor='r', alpha=0.2, edgecolor='r'))
    
    # smaller white borders
    plt.subplots_adjust(left=0, bottom=0.1, right=1, top=0.9, wspace=0, hspace=0)

    return plt 
\end{lstlisting}

The function \tn{plot\_results} returns a plot with an image and both predicted and ground truth bounding boxes 
and corresponding labels. The ground truths are marked with green color and predictions with red. Objects are drawn using the package matplotlib.

\section{Visualisation}
 The notebook \tn{results\_visualization}.